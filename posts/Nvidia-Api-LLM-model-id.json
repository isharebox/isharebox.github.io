{"__ud_title":"Nvidia Api LLM model id","__ud_tags":["ai","vibecoding"],"__ud_update_time":1771068064115,"__ud_create_time":1770548240812,"__ud_draft":false,"type":"doc","content":[{"type":"heading","attrs":{"level":1,"id":"Nvidia-Api-LLM-model-id"},"content":[{"type":"text","text":"Nvidia Api LLM model id"}]},{"type":"paragraph","content":[{"type":"text","text":"NVIDIA build models , here are the model IDs organized by category: Large Language Models (LLMs)"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"nvidia/nemotron-3-nano-30b-a3b - Open, efficient MoE model with 1M context\ndeepseek-ai/deepseek-v3.2 - 685B reasoning LLM with sparse attention\ndeepseek-ai/deepseek-v3.1 - Hybrid AI model with fast reasoning\nminimaxai/minimax-m2 - 230B MoE model (10B active)\nqwen/qwen3-235b-a22b - Advanced reasoning MoE model\nqwen/qwq-32b - Powerful reasoning model\nmistralai/mistral-nemotron - Built for agentic workflows\nigenius/colosseum_355b_instruct_16k - Multilingual LLM for regulated industries\ntiiuae/falcon3-7b-instruct - Instruction tuned LLM\nigenius/italia_10b_instruct_16k - European languages focused LLM\nmeta/llama-3.1-405b-instruct - Advanced LLM for synthetic data generation\nmeta/llama-3.1-8b-instruct - State-of-the-art model\nmeta/llama3-70b-instruct - Complex conversations model\nmeta/llama3-8b-instruct - Advanced LLM\nmeta/llama-3.2-3b-instruct - Small language model\nmeta/llama-3.2-1b-instruct - Small language model\nqwen/qwen2-7b-instruct - Chinese and English LLM\nthudm/chatglm3-6b - Chinese and English chatbot\ngoogle/gemma-2-27b-it - Text generation model\ngoogle/gemma-2-9b-it - Text generation model\ngoogle/gemma-7b - Text generation model\nrakuten/rakutenai-7b-chat - Advanced LLM"}]},{"type":"paragraph","content":[{"type":"text","text":"Vision-Language Models"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"nvidia/nemotron-parse - Vision-language model for text/metadata extraction\nnvidia/nemotron-nano-12b-v2-vl - Multi-image and video understanding\nnvidia/cosmos-reason2-8b - Physical world understanding\ngoogle/gemma-3n-e4b-it - Edge computing AI with multimodal input\ngoogle/gemma-3n-e2b-it - Edge computing AI with multimodal input\ngoogle/gemma-3-27b-it - Multimodal model for image reasoning\ngoogle/paligemma - Vision language model\nnvidia/nv-clip - Multimodal embeddings model"}]},{"type":"paragraph","content":[{"type":"text","text":"Scientific/Bio Models"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"openfold/openfold3 - Biomolecular foundation model\nopenfold/openfold2 - Protein 3D structure prediction\nmit/boltz-2 - Complex structure prediction\narc/evo2-40b - Biological foundation model\nmeta/esm2-650m - Protein embeddings\nmeta/esmfold - Protein 3D structure prediction\nipd/proteinmpnn - Amino acid sequence prediction\nipd/rfdiffusion - Protein backbone generation\nmit/diffdock - Molecule-protein interaction prediction\nnvidia/maisi - 3D CT Latent Diffusion model\nnvidia/genmol - Molecular generation\nnvidia/molmim-generate - Controlled molecular generation"}]},{"type":"paragraph","content":[{"type":"text","text":"Autonomous Driving"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"nvidia/streampetr - 3D object detection\nnvidia/sparsedrive - End-to-end autonomous driving stack\nnvidia/bevformer - Bird's-eye-view 3D perception"}]},{"type":"paragraph","content":[{"type":"text","text":"3D/Computer Vision"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"microsoft/trellis - 3D asset generation\nnvidia/cosmos-transfer1-7b - Physics-aware video generation\nnvidia/cosmos-predict1-5b - Future frame prediction\nnvidia/nv-dinov2 - Visual foundation model\nnvidia/visual-changenet - Change detection between images"}]},{"type":"paragraph","content":[{"type":"text","text":"Specialized Models"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"nvidia/nemoretriever-ocr - OCR for text extraction\nnvidia/nv-embedcode-7b-v1 - Code retrieval embeddings\nnvidia/llama-3.2-nv-embedqa-1b-v2 - Question-answering retrieval\nnvidia/usdcode - OpenUSD code generation\nnvidia/usdsearch - OpenUSD asset search\nnvidia/usdvalidate - OpenUSD asset validation\nnvidia/audio2face-3d - Audio to facial blendshapes\nnvidia/eyecontact - Gaze angle estimation\nnvidia/studiovoice - Speech enhancement\nnvidia/vista-3d - 3D medical imaging segmentation\nnvidia/ocdrnet - Optical character detection/recognition\nnvidia/corrdiff - Weather field generation\nnvidia/fourcastnet - Atmospheric dynamics prediction\nnvidia/cuopt - Route optimization"}]},{"type":"paragraph","content":[{"type":"text","text":"Other Models"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"baidu/paddleocr - Table extraction and OCR\nbaai/bge-m3 - Text retrieval embeddings\nnvidia/rerank-qa-mistral-4b - Question-answering reranking\ncolabfold/msa-search - Protein sequence alignment\nuniversity-at-buffalo/cached - Chart element detection"}]},{"type":"paragraph","content":[{"type":"text","text":"These models are available through NVIDIA's NIM (NVIDIA Inference Microservices) APIs for deployment and inference."}]},{"type":"paragraph","content":[{"type":"text","text":"Full list: "},{"type":"text","marks":[{"type":"link","attrs":{"href":"https://build.nvidia.com/models","target":"_blank","rel":"nofollow","class":null}}],"text":"build.nvidia.com/models"}]},{"type":"paragraph","content":[{"type":"text","marks":[{"type":"link","attrs":{"href":"https://api.chatanywhere.tech","target":"_blank","rel":"noreferer","class":null}}],"text":"https://api.chatanywhere.tech"}]},{"type":"paragraph","content":[{"type":"text","marks":[{"type":"link","attrs":{"href":"https://chatanywhere.apifox.cn/doc-5547696","target":"_blank","rel":"noopener noreferrer nofollow","class":null}}],"text":"https://chatanywhere.apifox.cn/doc-5547696"}]},{"type":"codeBlock","attrs":{"language":null},"content":[{"type":"text","text":"Model \tDescription\nminimaxai/minimax-m2.1 \tMinimax latest, strong Chinese capability\nz-ai/glm4.7 \tZhipu GLM4, fast response\nmeta/llama-3.3-70b-instruct \tMeta Llama 3.3 70B\nmeta/llama-3.1-405b-instruct \tMeta Llama 3.1 405B\ndeepseek-ai/deepseek-r1 \tDeepSeek R1 reasoning model\nqwen/qwen2.5-72b-instruct \tAlibaba Qwen 2.5\n"}]}]}